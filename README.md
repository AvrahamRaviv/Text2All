# Text2All    ![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square)
<!-- ![](https://img.shields.io/github/stars/AvrahamRaviv?affiliations=OWNER&color=YellowGreen&label=Stars&logo=StarsC&logoColor=StarsC&style=social) -->

> A comprehensive list of resources about text-guided generative models.
<p align="center">
<img src="https://github.com/AvrahamRaviv/Text2All/blob/main/TextToAll.png" width="512" height="512">
</p>

## Table of contents
- [Works and Papers](#works-and-papers)
  - [Text-to-Image](#text-to-image)
  - [Text-to-Video](#text-to-video)
  - [Text-to-3D](#text-to-3d)
  - [Text-to-Audio](#text-to-audio)
  - [Text-to-Motion](#text-to-motion)
  - [Text-to-Style](#text-to-style)

- [Tutorials](#tutorials)
  - [Resources](#resources)
  - [Blogs and Summaries](#blogs-and-summaries)
  - [Videos](#videos)

## Works and Papers

### Text-to-Image
- [DALL-E - Hierarchical Text-Conditional Image Generation with CLIP Latents](https://openai.com/dall-e-2/)

- [Imagen - Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding](https://imagen.research.google/)

- [Stable Diffusion - High-Resolution Image Synthesis with Latent Diffusion Models](https://github.com/CompVis/stable-diffusion)

- Stable Diffusion Notebooks: [Image](https://colab.research.google.com/github/deforum/stable-diffusion/blob/main/Deforum_Stable_Diffusion.ipynb?fbclid=IwAR23pz-LB_UcXOE1vBGIf6niGL86CHlISFhr4kfqYA-qUJR_m0EVfWOpg5Y), [Animation](https://colab.research.google.com/gist/costiash/d9421a1d4c0c66f8fb1d0f107b0c0bcb/stable-diffusion-animation-demo.ipynb?fbclid=IwAR23gmrl-TRW37PRxWTYlrixdy8DA3woxEpmaE62OzsZ48-dx7raraD7UAY), [Panorama](https://colab.research.google.com/drive/1RXRrkKUnpNiPCxTJg0Imq7sIM8ltYFz2?usp=sharing)

- [MidJourney](https://www.midjourney.com/home/)

- [GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models](https://github.com/openai/glide-text2im)

- [Parti - Pathways Autoregressive Text-to-Image](https://parti.research.google/)

- [AttnGAN: Fine-Grained Text to Image Generation with Attentional Generative Adversarial Networks](https://github.com/taoxugit/AttnGAN)

- [Imagic: Text-Based Real Image Editing with Diffusion Models](https://arxiv.org/pdf/2210.09276.pdf)

- [LAFITE: Towards Language-Free Training for Text-to-Image Generation](https://github.com/drboog/Lafite)

- [DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation](https://dreambooth.github.io/)

- [Make-A-Scene: Scene-Based Text-to-Image Generation with Human Priors](https://arxiv.org/abs/2203.13131)

- [https://text2live.github.io/?utm_source=catalyzex.com](https://text2live.github.io/?utm_source=catalyzex.com)

- [StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks](https://github.com/hanzhanggit/StackGAN)

### Text-to-Video
- [Make-A-Video: Text-to-Video Generation without Text-Video Data](https://makeavideo.studio/)

- [Imagen Video: High Definition Video Generation With Diffusion Models](https://imagen.research.google/video/)

- [Phenaki: Variable Length Video Generation from Open Domain Textual Descriptions](https://phenaki.video/?fbclid=IwAR2PPsn9kT7WGbOaTrr-Fi7UBVBWd8-BZzX3bLFT9B_WISO9LBGq8mBIl6M)

- [GODIVA: Generating Open-DomaIn Videos from nAtural Descriptions](https://arxiv.org/pdf/2104.14806v1.pdf)

- [CogVideo: Large-scale Pretraining for Text-to-Video Generation via Transformers](https://github.com/THUDM/CogVideo)

- [NÃœWA: Visual Synthesis Pre-training for Neural visUal World creAtion](https://github.com/microsoft/NUWA)

### Text-to-3D
- [DreamFusion: Text-to-3D using 2D Diffusion](https://dreamfusionpaper.github.io/)

- [Zero-Shot Text-Guided Object Generation with Dream Fields](https://ajayj.com/dreamfields)

- [CLIP-Mesh: Generating textured meshes from text using pretrained image-text models](https://www.nasir.lol/clipmesh)

### Text-to-Audio
- [AudioGen: Textually Guided Audio Generation](https://felixkreuk.github.io/text2audio_arxiv_samples/)

- [Diffsound: Discrete Diffusion Model for Text-to-sound Generation](http://dongchaoyang.top/text-to-sound-synthesis-demo/)

### Text-to-Motion
- [MotionCLIP: Exposing Human Motion Generation to CLIP Space](https://guytevet.github.io/motionclip-page/)

- [Human Motion Diffusion Model](https://guytevet.github.io/mdm-page/)

- [Language2Pose: Natural Language Grounded Pose Forecasting](https://arxiv.org/abs/1907.01108)

### Text-to-Style
- [Text-Driven Stylization of Video Objects](https://sloeschcke.github.io/Text-Driven-Stylization-of-Video-Objects/)

## Tutorials

### Resources

- [Awesome AI image synthesis - A list of awesome tools, ideas, prompt engineering tools, colabs, models, and helpers for the prompt designer playing with aiArt and image synthesis](https://github.com/altryne/awesome-ai-art-image-synthesis)

- [Tools and Resources for AI Art](https://pharmapsychotic.com/tools.html)

### Blogs and Summaries

- [The Illustrated Stable Diffusion](https://jalammar.github.io/illustrated-stable-diffusion/)

- [Introduction to Diffusion Models](https://www.assemblyai.com/blog/diffusion-models-for-machine-learning-introduction/)

- [How DALL-E works](https://www.assemblyai.com/blog/how-dall-e-2-actually-works/)

- [How Imagen works](https://www.assemblyai.com/blog/how-imagen-actually-works/)

- [Build Your Own Imagen](https://www.assemblyai.com/blog/minimagen-build-your-own-imagen-text-to-image-model/)

### Videos

- [Yannic Kilcher - Text-to-Image models are taking over! (Imagen, DALL-E 2, Midjourney, CogView 2 & more)](https://www.youtube.com/watch?v=af6WPqvzjjk&ab_channel=YannicKilcher)

- [Yannic Kilcher - Stable Diffusion Takes Over!](https://www.youtube.com/watch?v=xbxe-x6wvRw&ab_channel=YannicKilcher)
